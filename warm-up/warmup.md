link: https://openreview.net/forum?id=B1x8anVFPr
title: On Layer Normalization in the Transformer Architecture

# 作者背景
    作者是哪个学校的？隶属于什么组？
        中国科学院计算技术研究所，Ruibin Xiong
        北京大学，Yunchang Yang

    他们历史上都干了什么？在做什么方向，发了什么好ppaper？
        研一硕士研究生加本科生合作的paper，论文写的很全面。nlp方向，都只有一篇文章。


# 问题背景
    为什么这个问题（论文要handle的）是重要的？
        一篇对于warm-up训练trick的分析，同时对比了一种不需要warm-up的改进模型。一篇回顾分析性论文。

    问题的大背景和小背景是什么？
        基于transformer的nlp模型在许多任务上取得了很好的成绩，训练一个高性能的transformer model的过程warm-up通常是必须的，而且warm-up超参数的变化对模型性能影响很大。

# 提出的方法
    已有的方法有什么？
    warm-up

    为什么已有的方法不能（很好）解决这个问题？----
    可以但模型复杂，比较难训练，需要调参。

    为什么他的方法就能解决的好？

    他的方法核心亮点是什么？
    缺点是什么？牺牲了什么？
    论文的方法对这个问题的解决提供什么正反冲击？

# 实现的结果
    论文如何证明这个方法的有效性的？
    我认为他的方法是否可靠？
    如果是我有更好的方法吗？我的方法得失？

# 恰当总结
    对相关领域的影响
    对我研究的借鉴意义
    论文行文、画图、实验、总结、分析、设计等方面有何可取之处？
    会议风格，对我们投稿的指导意义